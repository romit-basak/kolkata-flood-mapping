{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba11505-569c-4920-9416-b444a8be9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently flooded wards (2023-2025):\n",
      "    ward  flood_count\n",
      "22   130            7\n",
      "6     68            7\n",
      "15    93            7\n",
      "3     65            6\n",
      "4     66            6\n",
      "14    91            5\n",
      "2     63            5\n",
      "5     67            5\n",
      "7     73            5\n",
      "19   107            5\n",
      "\n",
      "Dataset Summary:\n",
      "Total records: 184\n",
      "Unique dates: 8\n",
      "Flooded instances: 104\n",
      "Non-flooded instances: 80\n",
      "\n",
      "High-risk wards (flooded >50% of events):\n",
      "[12, 46, 63, 65, 66, 67, 68, 73, 74, 81, 84, 87, 88, 90, 91, 93, 96, 99, 100, 107, 110, 111, 130]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create comprehensive flood events dataset (2023-2025)\n",
    "all_events = []\n",
    "\n",
    "# 2023 Events\n",
    "events_2023 = [\n",
    "    {'date': '2023-09-02', 'wards': [68, 93]},\n",
    "]\n",
    "\n",
    "# 2024 Events  \n",
    "events_2024 = [\n",
    "    {'date': '2024-05-26', 'wards': [65, 68, 130, 93, 96, 99, 81, 74]},\n",
    "    {'date': '2024-07-08', 'wards': [93, 65, 68, 73, 84, 87, 88, 67, 91, 107, 100, 110, 111, \n",
    "                                      93, 96, 99, 130, 12, 90, 91, 63, 66]},\n",
    "    {'date': '2024-09-23', 'wards': [100, 110, 111, 93, 73, 84, 87, 88, 66, 65, 68, 130, \n",
    "                                      93, 96, 99, 81, 63, 46, 12, 67, 91, 107]},\n",
    "    {'date': '2024-10-25', 'wards': [73, 46, 130, 66]},\n",
    "]\n",
    "\n",
    "# 2025 Events\n",
    "events_2025 = [\n",
    "    {'date': '2025-07-08', 'wards': [12, 63, 67, 91, 107, 90, 91, 93, 65, 68, 130, 93, 96, \n",
    "                                      99, 111, 66, 100, 110]},\n",
    "    {'date': '2025-09-23', 'wards': [111, 93, 73, 84, 87, 88, 66, 65, 68, 74, 130, 93, 96, \n",
    "                                      99, 68, 90, 91, 67, 107, 65, 66, 67, 63, 12, 73]},\n",
    "    {'date': '2025-10-10', 'wards': [63, 73, 84, 87, 88, 68, 74, 130, 93, 65, 67, 91, 107, \n",
    "                                      65, 66, 67]}\n",
    "]\n",
    "\n",
    "# Compile all events\n",
    "for event_list, year in [(events_2023, 2023), (events_2024, 2024), (events_2025, 2025)]:\n",
    "    for event in event_list:\n",
    "        # Get unique ward numbers\n",
    "        unique_wards = list(set(event['wards']))\n",
    "        for ward in unique_wards:\n",
    "            all_events.append({\n",
    "                'date': event['date'],\n",
    "                'ward': ward,\n",
    "                'flooded': 1\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "flood_df = pd.DataFrame(all_events)\n",
    "\n",
    "# Remove duplicates (same ward on same date)\n",
    "flood_df = flood_df.drop_duplicates(subset=['date', 'ward'])\n",
    "\n",
    "# Identify consistently flooding wards\n",
    "flood_frequency = flood_df.groupby('ward').size().reset_index(name='flood_count')\n",
    "print(\"Most frequently flooded wards (2023-2025):\")\n",
    "print(flood_frequency.sort_values('flood_count', ascending=False).head(10))\n",
    "\n",
    "# Add some non-flooded examples for each event\n",
    "# Assuming northern wards (1-40) generally don't flood as much\n",
    "non_flooded_wards = []\n",
    "for date in flood_df['date'].unique():\n",
    "    flooded_on_date = flood_df[flood_df['date'] == date]['ward'].tolist()\n",
    "    # Add some wards that didn't flood (from northern areas)\n",
    "    safe_wards = [w for w in range(1, 40) if w not in flooded_on_date]\n",
    "    # Sample 5-10 safe wards per event\n",
    "    for ward in np.random.choice(safe_wards, min(10, len(safe_wards)), replace=False):\n",
    "        non_flooded_wards.append({\n",
    "            'date': date,\n",
    "            'ward': ward,\n",
    "            'flooded': 0\n",
    "        })\n",
    "\n",
    "non_flooded_df = pd.DataFrame(non_flooded_wards)\n",
    "\n",
    "# Combine flooded and non-flooded\n",
    "complete_flood_data = pd.concat([flood_df, non_flooded_df], ignore_index=True)\n",
    "\n",
    "# Sort by date and ward\n",
    "complete_flood_data = complete_flood_data.sort_values(['date', 'ward'])\n",
    "\n",
    "# Save comprehensive dataset\n",
    "complete_flood_data.to_csv('../data/processed/kolkata_flood_events_2023_2025.csv', index=False)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Total records: {len(complete_flood_data)}\")\n",
    "print(f\"Unique dates: {complete_flood_data['date'].nunique()}\")\n",
    "print(f\"Flooded instances: {complete_flood_data['flooded'].sum()}\")\n",
    "print(f\"Non-flooded instances: {(complete_flood_data['flooded'] == 0).sum()}\")\n",
    "\n",
    "# Identify high-risk wards (flooded in >50% of events)\n",
    "total_events = complete_flood_data['date'].nunique()\n",
    "ward_flood_rate = complete_flood_data.groupby('ward')['flooded'].mean()\n",
    "high_risk_wards = ward_flood_rate[ward_flood_rate > 0.5].index.tolist()\n",
    "\n",
    "print(f\"\\nHigh-risk wards (flooded >50% of events):\")\n",
    "print(sorted(high_risk_wards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651fa2d-e22f-4867-b46b-c9ec99597503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
